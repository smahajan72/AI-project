
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: solution.ipynb

import numpy as np
from helper_functions import *

def get_initial_means(array, k):
    """
    Picks k random points from the 2D array
    (without replacement) to use as initial
    cluster means

    params:
    array = numpy.ndarray[numpy.ndarray[float]] - m x n | datapoints x features

    k = int

    returns:
    initial_means = numpy.ndarray[numpy.ndarray[float]]
    """
    # TODO: finish this function͏︆͏󠄃͏󠄌͏󠄍͏󠄂͏️͏︊͏󠄅͏︉

    #flatten = array.flatten()
    randomIndices = np.random.choice(len(array), k, replace = False)
    randomPoints = array[randomIndices]
    return randomPoints
    #randomPoints.reshape((k, array.shape[1]))



    #raise NotImplementedError()

########## DON'T WRITE ANY CODE OUTSIDE THE FUNCTION! ################͏︆͏󠄃͏󠄌͏󠄍͏󠄂͏️͏︊͏󠄅͏︉
##### CODE BELOW IS USED FOR RUNNING LOCAL TEST DON'T MODIFY IT ######͏︆͏󠄃͏󠄌͏󠄍͏󠄂͏️͏︊͏󠄅͏︉
################ END OF LOCAL TEST CODE SECTION ######################͏︆͏󠄃͏󠄌͏󠄍͏󠄂͏️͏︊͏󠄅͏︉

def k_means_step(X, k, means):
    """
    A single update/step of the K-means algorithm
    Based on a input X and current mean estimate,
    predict clusters for each of the pixels and
    calculate new means.
    params:
    X = numpy.ndarray[numpy.ndarray[float]] - m x n | pixels x features (already flattened)
    k = int
    means = numpy.ndarray[numpy.ndarray[float]] - k x n

    returns:
    (new_means, clusters)
    new_means = numpy.ndarray[numpy.ndarray[float]] - k x n
    clusters = numpy.ndarray[int] - m sized vector
    """
    # TODO: finish this function͏︆͏󠄃͏󠄌͏󠄍͏󠄂͏️͏︊͏󠄅͏︉

    # initialize distances between pixels and center of cluster
    distances = np.zeros((X.shape[0], k))

    #iterate thru number of clusters and calculates euclidean distance - use np.linalg.norm to calculate distance btwn each point and mean
    for i in range(k):
        distances[:, i] = np.linalg.norm(X - means[i], axis =1)


    # assign to clusters the pixels w the smallest euclidean distance (look at column)
    clusters = np.argmin(distances, axis = 1)

    # initialize the new_means
    new_means = np.zeros((k, X.shape[1]))

    # reassign the means for the pixels with the minimum euclidean distances (that is held in clusters) (look at column)
    for i in range(k):
        new_means[i] = np.mean(X[clusters == i], axis = 0)

    return new_means, clusters






    #raise NotImplementedError()

########## DON'T WRITE ANY CODE OUTSIDE THE FUNCTION! ################͏︆͏󠄃͏󠄌͏󠄍͏󠄂͏️͏︊͏󠄅͏︉
##### CODE BELOW IS USED FOR RUNNING LOCAL TEST DON'T MODIFY IT ######͏︆͏󠄃͏󠄌͏󠄍͏󠄂͏️͏︊͏󠄅͏︉
################ END OF LOCAL TEST CODE SECTION ######################͏︆͏󠄃͏󠄌͏󠄍͏󠄂͏️͏︊͏󠄅͏︉

def k_means_segment(image_values, k=3, initial_means=None):
    """
    Separate the provided RGB values into
    k separate clusters using the k-means algorithm,
    then return an updated version of the image
    with the original values replaced with
    the corresponding cluster values.

    params:
    image_values = numpy.ndarray[numpy.ndarray[numpy.ndarray[float]]] - r x c x ch
    k = int
    initial_means = numpy.ndarray[numpy.ndarray[float]] or None

    returns:
    updated_image_values = numpy.ndarray[numpy.ndarray[numpy.ndarray[float]]] - r x c x ch
    """

    # reshape image_values to r*c, ch
    iv = flatten_image_matrix(image_values)
    # get initial means if you do not have
    if initial_means is None:
        initial_means = get_initial_means(iv, k)
    # find new_means, clusters. convergence test - when new means and initial means are equal
    means = initial_means.copy()
    isDifferent = True
    while isDifferent == True:
        new_means, clusters = k_means_step(iv, k, means)
        if np.allclose(new_means,means):
            isDifferent = False;
        means = new_means

    # updated version of image with corresponding cluster value s
    updated_image_values = np.zeros_like(iv)
    for i in range(k):
        updated_image_values[clusters == i] = means[i]

    #reshape back to image_values shape
    updated_image_values = unflatten_image_matrix(updated_image_values, image_values.shape[1])

    return updated_image_values


    #raise NotImplementedError()


########## DON'T WRITE ANY CODE OUTSIDE THE FUNCTION! ################͏︆͏󠄃͏󠄌͏󠄍͏󠄂͏️͏︊͏󠄅͏︉
##### CODE BELOW IS USED FOR RUNNING LOCAL TEST DON'T MODIFY IT ######͏︆͏󠄃͏󠄌͏󠄍͏󠄂͏️͏︊͏󠄅͏︉
################ END OF LOCAL TEST CODE SECTION ######################͏︆͏󠄃͏󠄌͏󠄍͏󠄂͏️͏︊͏󠄅͏︉

def compute_sigma(X, MU):
    """
    Calculate covariance matrix, based in given X and MU values

    params:
    X = numpy.ndarray[numpy.ndarray[float]] - m x n
    MU = numpy.ndarray[numpy.ndarray[float]] - k x n

    returns:
    SIGMA = numpy.ndarray[numpy.ndarray[numpy.ndarray[float]]] - k x n x n
    """
    #print(X.shape[0]) #m
    #print(X.shape[1]) #n
    #print(MU.shape[0]) #k
    SIGMA = np.zeros((MU.shape[0], X.shape[1], X.shape[1]))
    for i in range(MU.shape[0]):
        differences = X[:] - MU[i]
        SIGMA[i] = differences.T @ differences / X.shape[0]
        #np.matmul(differences.T, differences) / X.shape[0]

    return SIGMA

    # TODO: finish this function͏︆͏󠄃͏󠄌͏󠄍͏󠄂͏️͏︊͏󠄅͏︉
    #raise NotImplementedError()

########## DON'T WRITE ANY CODE OUTSIDE THE FUNCTION! ################͏︆͏󠄃͏󠄌͏󠄍͏󠄂͏️͏︊͏󠄅͏︉
##### CODE BELOW IS USED FOR RUNNING LOCAL TEST DON'T MODIFY IT ######͏︆͏󠄃͏󠄌͏󠄍͏󠄂͏️͏︊͏󠄅͏︉
################ END OF LOCAL TEST CODE SECTION ######################͏︆͏󠄃͏󠄌͏󠄍͏󠄂͏️͏︊͏󠄅͏︉

def initialize_parameters(X, k):
    """
    Return initial values for training of the GMM
    Set component mean to a random
    pixel's value (without replacement),
    based on the mean calculate covariance matrices,
    and set each component mixing coefficient (PIs)
    to a uniform values
    (e.g. 4 components -> [0.25,0.25,0.25,0.25]).

    params:
    X = numpy.ndarray[numpy.ndarray[float]] - m x n
    k = int

    returns:
    (MU, SIGMA, PI)
    MU = numpy.ndarray[numpy.ndarray[float]] - k x n
    SIGMA = numpy.ndarray[numpy.ndarray[numpy.ndarray[float]]] - k x n x n
    PI = numpy.ndarray[float] - k
    """

    MU = get_initial_means(X, k)
    SIGMA = compute_sigma(X, MU)
    PI = np.ones(k)/k

    return (MU, SIGMA, PI)






    # TODO: finish this function͏︆͏󠄃͏󠄌͏󠄍͏󠄂͏️͏︊͏󠄅͏︉
    # Hint: for initializing SIGMA you could choose to use compute_sigma͏︆͏󠄃͏󠄌͏󠄍͏󠄂͏️͏︊͏󠄅͏︉
    #raise NotImplementedError()

########## DON'T WRITE ANY CODE OUTSIDE THE FUNCTION! ################͏︆͏󠄃͏󠄌͏󠄍͏󠄂͏️͏︊͏󠄅͏︉
##### CODE BELOW IS USED FOR RUNNING LOCAL TEST DON'T MODIFY IT ######͏︆͏󠄃͏󠄌͏󠄍͏󠄂͏️͏︊͏󠄅͏︉
################ END OF LOCAL TEST CODE SECTION ######################͏︆͏󠄃͏󠄌͏󠄍͏󠄂͏️͏︊͏󠄅͏︉

def prob(x, mu, sigma):
    """Calculate the probability of x (a single
    data point or an array of data points) under the
    component with the given mean and covariance.
    The function is intended to compute multivariate
    normal distribution, which is given by N(x;MU,SIGMA).

    params:
    x = numpy.ndarray[float] (for single datapoint)
        or numpy.ndarray[numpy.ndarray[float]] (for array of datapoints)
    mu = numpy.ndarray[float]
    sigma = numpy.ndarray[numpy.ndarray[float]]

    returns:
    probability = float (for single datapoint)
                or numpy.ndarray[float] (for array of datapoints)
    """

    # number of rows for x (or 1 if a single data point)
    n = x.shape[0]
    #print(n)
    #print(x.shape)

    #calculating the coefficient


    if x.ndim == 1:
        coef = 1/(((2*np.pi)**(x.shape[0]/2)) * (np.linalg.det(sigma)**(1/2)))
        difference = x - mu
        inverse = np.linalg.inv(sigma)
        exp1 = np.dot(difference, inverse)
        exp2 = np.dot(exp1, difference)
        exp2 = -.5 * exp2
        probability = coef * np.exp(exp2)
        #print(probability)
    else :
        coef = 1/(((2*np.pi)**(x.shape[1]/2)) * (np.linalg.det(sigma)**(1/2)))
        difference = x - mu
        inverse = np.linalg.inv(sigma)
        exp1 = np.dot(difference, inverse)
        exp2 = exp1 * difference
        exp2 = np.sum(exp2, axis = 1)
        exp2 = -.5 * exp2
        probability = coef * np.exp(exp2)
        #print(probability)

    return probability


    # TODO: finish this function͏︆͏󠄃͏󠄌͏󠄍͏󠄂͏️͏︊͏󠄅͏︉
    #raise NotImplementedError()

########## DON'T WRITE ANY CODE OUTSIDE THE FUNCTION! ################͏︆͏󠄃͏󠄌͏󠄍͏󠄂͏️͏︊͏󠄅͏︉
##### CODE BELOW IS USED FOR RUNNING LOCAL TEST DON'T MODIFY IT ######͏︆͏󠄃͏󠄌͏󠄍͏󠄂͏️͏︊͏󠄅͏︉
################ END OF LOCAL TEST CODE SECTION ######################͏︆͏󠄃͏󠄌͏󠄍͏󠄂͏️͏︊͏󠄅͏︉

def E_step(X,MU,SIGMA,PI,k):
    """
    E-step - Expectation
    Calculate responsibility for each
    of the data points, for the given
    MU, SIGMA and PI.

    params:
    X = numpy.ndarray[numpy.ndarray[float]] - m x n
    MU = numpy.ndarray[numpy.ndarray[float]] - k x n
    SIGMA = numpy.ndarray[numpy.ndarray[numpy.ndarray[float]]] - k x n x n
    PI = numpy.ndarray[float] - k
    k = int

    returns:
    responsibility = numpy.ndarray[numpy.ndarray[float]] - k x m
    """

    sum = 0
    responsibility = np.zeros((MU.shape[0], X.shape[0]))
    for i in range(k):
        responsibility[i] = np.dot(PI[i], prob(X, MU[i], SIGMA[i]))
        sum += responsibility[i]

    #for j in range(k):
     #   responsibility = responsibility / sum

    return responsibility / sum

    # TODO: finish this function͏︆͏󠄃͏󠄌͏󠄍͏󠄂͏️͏︊͏󠄅͏︉
    #raise NotImplementedError()

########## DON'T WRITE ANY CODE OUTSIDE THE FUNCTION! ################͏︆͏󠄃͏󠄌͏󠄍͏󠄂͏️͏︊͏󠄅͏︉
##### CODE BELOW IS USED FOR RUNNING LOCAL TEST DON'T MODIFY IT ######͏︆͏󠄃͏󠄌͏󠄍͏󠄂͏️͏︊͏󠄅͏︉
################ END OF LOCAL TEST CODE SECTION ######################͏︆͏󠄃͏󠄌͏󠄍͏󠄂͏️͏︊͏󠄅͏︉

def M_step(X, r, k):
    """
    M-step - Maximization
    Calculate new MU, SIGMA and PI matrices
    based on the given responsibilities.

    params:
    X = numpy.ndarray[numpy.ndarray[float]] - m x n
    r = numpy.ndarray[numpy.ndarray[float]] - k x m
    k = int

    returns:
    (new_MU, new_SIGMA, new_PI)
    new_MU = numpy.ndarray[numpy.ndarray[float]] - k x n
    new_SIGMA = numpy.ndarray[numpy.ndarray[numpy.ndarray[float]]] - k x n x n
    new_PI = numpy.ndarray[float] - k
    """
    m = X.shape[0]
    n = X.shape[1]
    k = r.shape[0]

    Nc = np.sum(r, axis = 1)

    new_PI = Nc / m

    #print("my pi")
    #print(new_PI.shape)
    #new_PI = np.zeros(k)
    #for i in range(k):
        #new_PI[i] = np.sum(r[i])/m

    resp = np.zeros((k, n))
    for i in range(k):
        resp[i] = np.dot(r[i], X) / Nc[i]

    new_MU = resp


    #print(new_MU.shape)

    new_SIGMA = np.zeros((k,n,n))
    for i in range(k):
        difference = X - new_MU[i]
        diff2 = np.dot(r[i] * difference.T, difference)
        new_SIGMA[i] = diff2 / Nc[i]

    return new_MU, new_SIGMA, new_PI



    # TODO: finish this function͏︆͏󠄃͏󠄌͏󠄍͏󠄂͏️͏︊͏󠄅͏︉
    #raise NotImplementedError()

########## DON'T WRITE ANY CODE OUTSIDE THE FUNCTION! ################͏︆͏󠄃͏󠄌͏󠄍͏󠄂͏️͏︊͏󠄅͏︉
##### CODE BELOW IS USED FOR RUNNING LOCAL TEST DON'T MODIFY IT ######͏︆͏󠄃͏󠄌͏󠄍͏󠄂͏️͏︊͏󠄅͏︉
################ END OF LOCAL TEST CODE SECTION ######################͏︆͏󠄃͏󠄌͏󠄍͏󠄂͏️͏︊͏󠄅͏︉

def loglikelihood(X, PI, MU, SIGMA, k):
    """Calculate a log likelihood of the
    trained model based on the following
    formula for posterior probability:

    log(Pr(X | mixing, mean, stdev)) = sum((i=1 to m), log(sum((j=1 to k),
                                      mixing_j * N(x_i | mean_j,stdev_j))))

    Make sure you are using natural log, instead of log base 2 or base 10.

    params:
    X = numpy.ndarray[numpy.ndarray[float]] - m x n
    MU = numpy.ndarray[numpy.ndarray[float]] - k x n
    SIGMA = numpy.ndarray[numpy.ndarray[numpy.ndarray[float]]] - k x n x n
    PI = numpy.ndarray[float] - k
    k = int

    returns:
    log_likelihood = float
    """
    sum = 0
    probability = np.zeros((MU.shape[0], X.shape[0]))
    for i in range(k):
        probability[i] = np.dot(PI[i], prob(X, MU[i], SIGMA[i]))
        sum += probability[i]

    likelihood = np.sum(np.log(sum))

    #print("likelihood:")
    #print(likelihood)

    return likelihood

    # TODO: finish this function͏︆͏󠄃͏󠄌͏󠄍͏󠄂͏️͏︊͏󠄅͏︉
    #raise NotImplementedError()

########## DON'T WRITE ANY CODE OUTSIDE THE FUNCTION! ################͏︆͏󠄃͏󠄌͏󠄍͏󠄂͏️͏︊͏󠄅͏︉
##### CODE BELOW IS USED FOR RUNNING LOCAL TEST DON'T MODIFY IT ######͏︆͏󠄃͏󠄌͏󠄍͏󠄂͏️͏︊͏󠄅͏︉
################ END OF LOCAL TEST CODE SECTION ######################͏︆͏󠄃͏󠄌͏󠄍͏󠄂͏️͏︊͏󠄅͏︉

def train_model(X, k, convergence_function, initial_values = None):
    """
    Train the mixture model using the
    expectation-maximization algorithm.
    E.g., iterate E and M steps from
    above until convergence.
    If the initial_values are None, initialize them.
    Else it's a tuple of the format (MU, SIGMA, PI).
    Convergence is reached when convergence_function
    returns terminate as True,
    see default convergence_function example
    in `helper_functions.py`

    params:
    X = numpy.ndarray[numpy.ndarray[float]] - m x n
    k = int
    convergence_function = func
    initial_values = None or (MU, SIGMA, PI)

    returns:
    (new_MU, new_SIGMA, new_PI, responsibility)
    new_MU = numpy.ndarray[numpy.ndarray[float]] - k x n
    new_SIGMA = numpy.ndarray[numpy.ndarray[numpy.ndarray[float]]] - k x n x n
    new_PI = numpy.ndarray[float] - k
    responsibility = numpy.ndarray[numpy.ndarray[float]] - k x m
    """

    if initial_values == None:
        initial_values = initialize_parameters(X, k)

    MU = initial_values[0]
    SIGMA = initial_values[1]
    PI = initial_values[2]

    prevlogL = 0
    x = 0

    end = False
    while end == False:
        responsibility = E_step(X, MU, SIGMA, PI, k)
        new_mu, new_sigma, new_pi = M_step(X, responsibility, k)

        logL = loglikelihood(X, new_pi, new_mu, new_sigma, k)
        x, end = convergence_function(prevlogL, logL, x)
        #print(end)
        prevlogL = logL

        MU = new_mu
        SIGMA = new_sigma
        PI = new_pi
        print("LOGL")
        print(logL)




    return new_mu, new_sigma, new_pi, responsibility




    # TODO: finish this function͏︆͏󠄃͏󠄌͏󠄍͏󠄂͏️͏︊͏󠄅͏︉
    #raise NotImplementedError()

########## DON'T WRITE ANY CODE OUTSIDE THE FUNCTION! ################͏︆͏󠄃͏󠄌͏󠄍͏󠄂͏️͏︊͏󠄅͏︉
##### CODE BELOW IS USED FOR RUNNING LOCAL TEST DON'T MODIFY IT ######͏︆͏󠄃͏󠄌͏󠄍͏󠄂͏️͏︊͏󠄅͏︉
################ END OF LOCAL TEST CODE SECTION ######################͏︆͏󠄃͏󠄌͏󠄍͏󠄂͏️͏︊͏󠄅͏︉

def cluster(r):
    """
    Based on a given responsibilities matrix
    return an array of cluster indices.
    Assign each datapoint to a cluster based,
    on component with a max-likelihood
    (maximum responsibility value).

    params:
    r = numpy.ndarray[numpy.ndarray[float]] - k x m - responsibility matrix

    return:
    clusters = numpy.ndarray[int] - m x 1

    """
    return np.argmax(r, axis = 1)

    # TODO: finish this͏︆͏󠄃͏󠄌͏󠄍͏󠄂͏️͏︊͏󠄅͏︉
    #raise NotImplementedError()

########## DON'T WRITE ANY CODE OUTSIDE THE FUNCTION! ################͏︆͏󠄃͏󠄌͏󠄍͏󠄂͏️͏︊͏󠄅͏︉
##### CODE BELOW IS USED FOR RUNNING LOCAL TEST DON'T MODIFY IT ######͏︆͏󠄃͏󠄌͏󠄍͏󠄂͏️͏︊͏󠄅͏︉
################ END OF LOCAL TEST CODE SECTION ######################͏︆͏󠄃͏󠄌͏󠄍͏󠄂͏️͏︊͏󠄅͏︉

def segment(X, MU, k, r):
    """
    Segment the X matrix into k components.
    Returns a matrix where each data point is
    replaced with its max-likelihood component mean.
    E.g., return the original matrix where each pixel's
    intensity replaced with its max-likelihood
    component mean. (the shape is still mxn, not
    original image size)

    params:
    X = numpy.ndarray[numpy.ndarray[float]] - m x n
    MU = numpy.ndarray[numpy.ndarray[float]] - k x n
    k = int
    r = numpy.ndarray[numpy.ndarray[float]] - k x m - responsibility matrix

    returns:
    new_X = numpy.ndarray[numpy.ndarray[float]] - m x n
    """
    new_X = np.zeros((X.shape[0], X.shape[1]))

    for i in range(X.shape[0]):
        maxIndex = np.argmax(r[:, i])
        new_X[i] = MU[maxIndex]

    return new_X


    # TODO: finish this function͏︆͏󠄃͏󠄌͏󠄍͏󠄂͏️͏︊͏󠄅͏︉
    #raise NotImplementedError()

########## DON'T WRITE ANY CODE OUTSIDE THE FUNCTION! ################͏︆͏󠄃͏󠄌͏󠄍͏󠄂͏️͏︊͏󠄅͏︉
##### CODE BELOW IS USED FOR RUNNING LOCAL TEST DON'T MODIFY IT ######͏︆͏󠄃͏󠄌͏󠄍͏󠄂͏️͏︊͏󠄅͏︉
################ END OF LOCAL TEST CODE SECTION ######################͏︆͏󠄃͏󠄌͏󠄍͏󠄂͏️͏︊͏󠄅͏︉

def best_segment(X,k,iters):
    """Determine the best segmentation
    of the image by repeatedly
    training the model and
    calculating its likelihood.
    Return the segment with the
    highest likelihood.

    params:
    X = numpy.ndarray[numpy.ndarray[float]] - m x n
    k = int
    iters = int

    returns:
    (likelihood, segment)
    likelihood = float
    segment = numpy.ndarray[numpy.ndarray[float]]
    """

    MU, SIGMA, PI = initialize_parameters(X,k)

    maximum = 0
    maxI = 0
    prevlogL = 0
    logL = 0
    for i in range(iters):
        new_MU, new_SIGMA, new_PI, responsibility = train_model(X, k, default_convergence, (MU, SIGMA, PI))
        logL = loglikelihood(X, new_PI, new_MU, new_SIGMA, k)
        if logL > maximum:
            maximum = logL
            maxI = i

        PI = new_PI
        SIGMA = new_SIGMA
        MU = new_MU

    # finding the best segment given max likelihood and index that corresponds to max likelihood
    resp = E_step(X, MU, SIGMA, PI, k)
    new_x = segment(X, MU, k, resp)
    best_segment = new_x[maxI]

    print("log:")
    print(logL)


    return (maximum, best_segment)


    # TODO: finish this function͏︆͏󠄃͏󠄌͏󠄍͏󠄂͏️͏︊͏󠄅͏︉
    #raise NotImplementedError()

########## DON'T WRITE ANY CODE OUTSIDE THE FUNCTION! ################͏︆͏󠄃͏󠄌͏󠄍͏󠄂͏️͏︊͏󠄅͏︉
##### CODE BELOW IS USED FOR RUNNING LOCAL TEST DON'T MODIFY IT ######͏︆͏󠄃͏󠄌͏󠄍͏󠄂͏️͏︊͏󠄅͏︉
################ END OF LOCAL TEST CODE SECTION ######################͏︆͏󠄃͏󠄌͏󠄍͏󠄂͏️͏︊͏󠄅͏︉

def bayes_info_criterion(X, PI, MU, SIGMA, k):
    """
    See description above
    params:
    X = numpy.ndarray[numpy.ndarray[float]] - m x n
    MU = numpy.ndarray[numpy.ndarray[float]] - k x n
    SIGMA = numpy.ndarray[numpy.ndarray[numpy.ndarray[float]]] - k x n x n
    PI = numpy.ndarray[float] - k
    k = int

    return:
    bayes_info_criterion = int
    """
    #n = X.shape[1]
    logL = loglikelihood(X, PI, MU, SIGMA, k)
    k = k * (MU.shape[1] + SIGMA.shape[1]*(SIGMA.shape[1])/2 + 1)
    bayes_info_criterion = k * np.log(X.shape[0]) - 2 * logL

    print("BIC:")
    print(bayes_info_criterion)

    return bayes_info_criterion


    # TODO: finish this function͏︆͏󠄃͏󠄌͏󠄍͏󠄂͏️͏︊͏󠄅͏︉
    #raise NotImplementedError()

########## DON'T WRITE ANY CODE OUTSIDE THE FUNCTION! ################͏︆͏󠄃͏󠄌͏󠄍͏󠄂͏️͏︊͏󠄅͏︉
##### CODE BELOW IS USED FOR RUNNING LOCAL TEST DON'T MODIFY IT ######͏︆͏󠄃͏󠄌͏󠄍͏󠄂͏️͏︊͏󠄅͏︉
################ END OF LOCAL TEST CODE SECTION ######################͏︆͏󠄃͏󠄌͏󠄍͏󠄂͏️͏︊͏󠄅͏︉

def BIC_likelihood_model_test(image_matrix, comp_means):
    """Returns the number of components
    corresponding to the minimum BIC
    and maximum likelihood with respect
    to image_matrix and comp_means.

    params:
    image_matrix = numpy.ndarray[numpy.ndarray[float]] - m x n
    comp_means = list(numpy.ndarray[numpy.ndarray[float]]) - list(k x n) (means for each value of k)

    returns:
    (n_comp_min_bic, n_comp_max_likelihood)
    n_comp_min_bic = int
    n_comp_max_likelihood = int
    """

    MU, SIGMA, PI = initialize_parameters(image_matrix, len(comp_means))
    minBic = float('inf')
    maxLogL = float('-inf')
    for i in range(1, len(comp_means)):
        k = comp_means[i].shape[0] #number of clusters that you pass into following functions
        MU, SIGMA, PI, responsibility = train_model(image_matrix, k, default_convergence, (comp_means[i - 1], SIGMA, PI))
        bic = bayes_info_criterion(image_matrix, PI, comp_means[i-1], SIGMA, k)
        logL = loglikelihood(image_matrix, PI, comp_means[i-1], SIGMA, k)
        if (bic < minBic):
            minBic = bic
        if (logL > maxLogL):
            maxLogL = logL



    return min_bic, max_logL


    # TODO: finish this method͏︆͏󠄃͏󠄌͏󠄍͏󠄂͏️͏︊͏󠄅͏︉
    #raise NotImplementedError()

def return_your_name():
    # return your name͏︆͏󠄃͏󠄌͏󠄍͏󠄂͏️͏︊͏󠄅͏︉
    # TODO: finish this͏︆͏󠄃͏󠄌͏󠄍͏󠄂͏️͏︊͏󠄅͏︉
    return "Shreya"
    #raise NotImplemented()